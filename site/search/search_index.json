{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"@Home HRI Abstract Roborregos is the official robotics team of Tec de Monterrey, Monterrey campus. The @Home project consists of the development of a service robot for the home. An HRI (Human Robot Interaction) system allows the operators and developers of the robot to interact with it, both by providing manual inputs and receiving feedback. The purpose of this project is to develop an HRI that allows close introspection of the current @Home robot\u2019s sub-systems and good architecture modularity in case sub-systems are added or modified in the future. Introduction The term robot has been around for a long time and even though the definition applies to both physical things and abstract mechanisms that exist only in software, the general public associates the word robot with a machine, and generally, it is also associated with an industrial setting. This might have been true in the past but with the massive adoption of innovation by the public and the rapid technology advancements of the recent decades, the technologies that have made robots useful for the industry are being applied to commercial and residential use cases. The International Organization for Standardization (ISO) defines \u201cservice robots\u201d as \u201crobots that perform useful tasks for humans or equipment, but not for industrial automation applications\u201d. Service robots are very useful when you want to guarantee consistency in a task that involves humans. Humans can react to external factors in many different ways and consistency is hard to achieve, this is especially true in mundane jobs that are highly repetitive (Karunasena et al, 2021.). As robots moved from an industrial setting to performing tasks that are useful to humans, a necessity arose to include social interaction capabilities. If a robot cannot be accepted and be proficient in social scenarios it would be really difficult for them to be useful in applications involving humans. Humans are used to heads being the focal point when interacting with other humans, thus it is expected from robots to possess a responsive social interface to act as this focal point (McGinn, 2019). This project aims to create that interaction focal point while at the same time providing all the features necessary to monitor the robot during contests and development/debugging. Development The current version of the @Home software is developed using ROS as an overarching framework and both C++ and Python as languages for the individual subsystems. As previously stated, the HRI interface must be able to have deep introspection of the robot subsystems, this means that at least one of the components must run within the ROS environment or have access to the streams of information generated in it. As we\u2019ll see in the next section, this requirement led us to divide the HRI subsystem in several independent parts and deal with the challenges that this brought. System Architecture The system is composed of 3 independent components: An interface developed as a web application, a web server from which the interface retrieves the information to show and finally a ROS node that collects the data from the other subsystems and acts as a database for the web server. Figure 1 shows a high level overview of this architecture and how the communication between them is performed. There\u2019s two main communication scenarios that are showcased with the arrows: from any of the submodules of the robot to the HRI interface, or from the interface to one of the submodules. For the first case, the interface receives each and every message through the use of web sockets, for the second case, the interface sends messages through standard HTTP requests. In any of the two scenarios, the web server establishes communication with the HRI ROS node through the use of Listener and Client primitives that are implemented in the multiprocessing standard Python library, this provides process-to-process communication with built-in serialization using TCP/IP as the underlying protocols. The communications between the HRI ROS node and any other nodes that host different subsystems is done through ROS primitives, meaning using standard or custom ROS messages and by publishing to predefined topics. Figure 1. High level HRI system architecture The reason why the web app interface has to be separate from the other components is fairly obvious, the fact that it is built with a library meant to create SPAs (Single Page Applications) means that it\u2019s supposed to be served independently from the API. However, this does not explain why the web server (the API) has to be a separate process from the ROS node. Both of the components that were just mentioned were written in Python because of its ease of use and our previous familiarity with the language; Python is an interpreted language with dynamic typing which comes with its own set of pros and cons. An interpreted language, as opposed to a compiled language, is not directly executed by the CPU, instead it is either converted to an intermediate representation or kept the same, but in both cases it is executed by an interpreter which is also sometimes called a VM (virtual machine). With all of this said, the root cause of the separation is a mechanism of the Python interpreter called GIL (Global Interpreter Lock) which only allows the code of one thread to be executed concurrently; If ROS and Flask (the framework used to develop the web server) used the same concurrency model it probably wouldn\u2019t have been a problem. However, ROS uses multithreading for concurrency while Flask and almost all other web frameworks for Python use an event loop; these models clash and thus have to be isolated in different processes. Web application interface The interface is to be run in one of the computers that forms the robot\u2019s computing cluster, most likely running linux; any technology that allows building user interfaces and communication with a web server could suffice. We went with web technologies because of its portability which would allow us to run it in any other machine should it or its operating system change in the future; specifically, we opted to use ReactJS because of its simplicity and wide adoption in the community. Figure 2 shows the main and only page of the interface which is divided into different and independent UI modules. Each module is responsible for receiving information from the web server and displaying it when new updates are available. Each module occupies a flexible amount of screen size and is placed on a flowing grid that allows an arbitrary number of elements to be placed comfortably on the available space. Figure 2. Interface with all the current modules active. The current modules and their corresponding functionalities are listed in the table below. It is worth noting that the ability to easily add future modules is one of the core principles that guided the design of this system, meaning this table should grow easily as the needs of the operators and/or developers change. Name Functionality Communication direction Robot Face Emotionally reflect the actions that the robot is performing. From ROS to the interface. Video feed Show a direct or manipulated (labels, bounding boxes, etc.) feed of video. From ROS to the interface. System health Show the system status of the computing unit in the cluster that is running the interface. This could be extended to show the status of all the computing units. From ROS to the interface. Active robot modules Show which of the robot modules are currently doing processing. From ROS to the interface. Robot Chat Show a log of the text-to-speech messages the robot has emitted. Send custom commands bypassing the speech-to-text module. Both ways. Stop button Send a signal to the robot in case a manual stop is required. From the Interface to ROS. Table 1. UI modules of the HRI interface.. Figure 3. Interface drawer. Table 1 shows all the available modules and while all of them can certainly be on the screen at the same time, there\u2019s no need to have them all; figure 3 shows the system drawer that slides open when the bottom right button shown in figure 2 is pressed. The drawer allows you to select the modules that you wish to see on the main screen as well as creating new profiles or selecting existing ones; a profile is a locally saved configuration that groups several modules together, meaning, when a profile is selected all the modules it groups are shown on the interface while the ones it doesn\u2019t group are hidden. Figure 4. Profile selection UI. Figure 4 shows the UI that can be used to select one of the existing profiles; the interaction is really simple, the dropdown is clicked and when the profile is selected, only the UI modules corresponding to it are shown. When the Add config button is clicked, the modal shown in figure 5 is shown. Figure 5. Profile creation UI. The process to create a profile is very simple: First a name is chosen and then the modules that the profile is going to group must be selected, all or none modules can be selected without any limitations. Web server As mentioned previously, the web server is written in python and uses Flask as the framework of choice. It has a direct connection with the ROS node which can be observed in figure 6. # Full duplex connection with the HRI ros node. receiver_address = ('localhost', 7000) sender_address = ('localhost', 7001) Figure 6. Variables definition for full duplex communication from the server side. To establish a full duplex (or bidirectional) communication with the ros node we have to have both a sender and a receiver and that\u2019s precisely what is showcased in figure 6. The port used for the ROS receiver is 7000 while the port for the sender is 7001, both of these are ports in the local network, meaning in the same compute unit of the robot\u2019s cluster. Table 1 also shows that the stop button requires communication from the interface to the ROS subsystems and for this, the communication has to go through the web server in which HTTP is used. Figure 7 shows the route definition that provides the stop functionality. The interface sends a post request to the /stop endpoint and the web server uses the sender (which previously went through an initialization process) to tell the ros node to shut itself down. @app.route(\"/stop\", methods=[\"POST\"]) def stop_robot(): global ros_sender ros_sender.send(\"shutdown\") return jsonify({\"message\": \"Robot was shutdown\"}) Figure 7. Endpoint definition for the stop button. The other purpose of the web server is message forwarding in the opposite direction, meaning from the ROS subsystems to the interface. This is a well defined task that is performed in the function ros_receive_handler which is showcased in figure 8; the function runs as a task that is dispatched to the event loop (the concurrency models were explained previously). Since the ROS receiver is not initialized immediately, there\u2019s a mechanism against lockup that allows all events running in the loop to progress. The important part of this definition is the else clause of the inner if statement: Whenever the web server receives a message from the ROS node, it analyzes is content and in particular, when a certain format is followed, the web socket channel and the value to be sent are extracted and then emitted through the socket itself. # Decode messages and send them to the appropriate socket channel. def ros_receive_handler(): while True: if ros_receiver is None: # Avoid blocking the process before initialization. socketio.sleep(0.01) continue # Only call `recv` when we're sure there's a new message since # it is a blocking call. if ros_receiver.poll(): try: message = ros_receiver.recv() except: # Ignore message if there was an exception. # TODO: We should not do this becuase we could # loose critical messages. rospy.loginfo(\"There was an exception\") continue if message == \"CreateSender\": initialize_ros_sender() elif message == \"Close\": cleanup() else: channel = message[\"channel\"] value = message[\"value\"] socketio.emit( channel, value if type(value) == str else json.dumps(value) ) else: # Only throttle if there are no available messages. socketio.sleep(0.01) Figure 8. Definition of the event forwarding mechanism. ROS Node As previously mentioned, the role of the ROS node is to compile the information coming from the subsystems and make it easily available to the interface through the web server. In this node we also need the implementation of the full duplex communication; figure 9 shows that the definition is exactly the same, as are the ports to be used, the difference is that from the ROS side, the ports are interchanged: Port 7000 for the server is the receiver while in ROS is the sender; port 7001 for the server is the sender while in ROS is the receiver. # Implement a full duplex connection with the flask server. sender_address = ('localhost', 7000) receiver_address = ('localhost', 7001) Figure 9. Variables definition for full duplex communication from the ROS side. Similar to the server process, the ROS node has a message handler for anything that\u2019s coming from the web server that needs to be handled; for the current implementation, the only event that is handled by ROS coming from the server is shutdown. The implementation for this handler is shown in figure 10, as opposed to the server, the concurrency model in ROS is multithreaded and thus, flask_receive_handler is scheduled as a thread. def flask_receive_handler(): while True: if server_receiver is None: # Don't block execution. time.sleep(0.01) continue if server_receiver.poll(): try: message = server_receiver.recv() if message == \"shutdown\": rospy.loginfo(\"Stopping system\") rospy.loginfo(message) except: break # Don't block execution. time.sleep(.01) Figure 10. Message handler definition for messages going from the server to ROS. The types of messages that the interface currently handles could be classified in two: Robot status and video feed. The video feed is easily represented by a series of Image ROS messages; the definition for such messages is exported by the sensor_msgs package which is widely used by the community. In the case of robot status we created a custom message that encodes all the information needed by the interface (other than images), this definition is shown in figure 11. Header header string[] ActionQueue float32[3] SystemHealth string RobotStatus string[] ActiveModules string RobotMessage string RobotFace Figure 11. Definition for the RobotStatus custom message. As it has been previously mentioned, other than from the server, the ROS node must receive messages from several robot subsystems, and this is done differently for the two discussed cases (status or video). Figure 12 shows how the messages regarding status are handled, which if we analyze it carefully, is a simple forward: We extract the information from the RobotStatus message and send it to the web server. This function is used as the callback of a ROS subscription for the /robo_info topic, which internally is launched as a separate thread. def robot_info_receive_handler(robot_status): if server_sender is not None: for channel in channels: try: server_sender.send({ \"channel\": channel, \"value\": getattr(robot_status, channel) }) except: print(\"Error fetching: \", channel) ... channels = [ \"ActionQueue\", \"SystemHealth\", \"RobotStatus\", \"ActiveModules\", \"RobotMessage\", \"RobotFace\" ] Figure 12. Message handler for robot status The process for the camera feed is very similar, in this case the topic used is /robot_video_feed but there\u2019s some data transformation required before being forwarded to the server, all of this is showcased in figure 13. Originally, the stream arrives as Image messages but has to be encoded as base64; any and all messages sent to the interface through the web sockets must be in string form, either JSON encoded or as plain string data. Base 64 was chosen because browsers support it directly, that is to say, if we can send images as base64 encoded binaries, the browser that is rendering the interface can show them without any conversion. Luckily, OpenCV (which is already used in the robot) has built-in functions to obtain the binary value of an image which can in turn be encoded using Python standard libraries. def robot_video_feed_receive_handler(image_msg): if server_sender is not None: img = bridge.imgmsg_to_cv2(image_msg, desired_encoding=\"passthrough\") _, img_buffer = cv2.imencode('.jpg', img) img_text = base64.b64encode(img_buffer) server_sender.send({ \"channel\": \"CameraFeed\", \"value\": img_text.decode(\"ascii\") }) Figure 13. Message handler for the camera feed. Future work The present work has shown the architecture and implementation of an interface that serves as a proof of concept that such systems can be built and also as a first functional implementation, this because most of the requirements of the development team were met, at least referring to the interface. The future work for the HRI system is very clear: Integrate the interface with the ROS subsystems that can make use of it, specifically, create internal tasks in the nodes so they can send information they see fit to either the /robot_inf or /robot_video_feed topics. Additionally, new UI modules can be added to the interface as needs arise. However, this requires a complete process involving all 3 of the modules. Future iterations of the interface could make the process to add modules easier or even automated through manual scripts or code generation techniques. Conclusion We believe that the way in which we interact with technology and in this case, how we interact with robots, will be a very big research and development area in the future. Although there are multiple classes of robots that can benefit from a better or more robust interaction, the service robot category will certainly receive more attention since there\u2019s still a lot to improve. The purpose of this project was to prove that these types of systems can be built with current technology, for the case of the @Home robot this meant using ROS, Python and web technologies. With a few hiccups along the way, we ended up with a functional first version that paves the way for future development by the end users themselves (the Roborregos team). While the work performed here showcases a very small part of the problems to be solved in this discipline (integration of current technologies), it sets the precedents for future development and showcases the importance of having a system of such kind. References Karunasena, Ramesha et al. \u201cDEVI: Open-Source Human-Robot Interface for Interactive Receptionist Systems.\u201d 2019 IEEE 4th International Conference on Advanced Robotics and Mechatronics (ICARM) (2019): n. pag. Crossref. Web. McGinn, Conor. \u201cWhy Do Robots Need a Head? The Role of Social Interfaces on Service Robots.\u201d International Journal of Social Robotics, vol. 12, no. 1, 2020, p. 281. EBSCOhost, doi:10.1007/s12369-019-00564-5. Iocchi, L. (. 1. )., et al. \u201cRoboCup@Home: Analysis and Results of Evolving Competitions for Domestic and Service Robots.\u201d Artificial Intelligence, vol. 229, pp. 258\u2013281. EBSCOhost, doi:10.1016/j.artint.2015.08.002. Accessed 10 June 2021. Stalljann, Sarah et al. \u201cPerformance Analysis of a Head and Eye Motion-Based Control Interface for Assistive Robots.\u201d Sensors 20.24 (2020): 7162. Crossref. Web. Chen, Mingxuan, et al. \u201cA Human\u2013robot Interface for Mobile Manipulator.\u201d Intelligent Service Robotics, vol. 11, no. 3, 2018, p. 269. EBSCOhost, doi:10.1007/s11370-018-0251-3.","title":"@Home HRI"},{"location":"#home-hri","text":"","title":"@Home HRI"},{"location":"#abstract","text":"Roborregos is the official robotics team of Tec de Monterrey, Monterrey campus. The @Home project consists of the development of a service robot for the home. An HRI (Human Robot Interaction) system allows the operators and developers of the robot to interact with it, both by providing manual inputs and receiving feedback. The purpose of this project is to develop an HRI that allows close introspection of the current @Home robot\u2019s sub-systems and good architecture modularity in case sub-systems are added or modified in the future.","title":"Abstract"},{"location":"#introduction","text":"The term robot has been around for a long time and even though the definition applies to both physical things and abstract mechanisms that exist only in software, the general public associates the word robot with a machine, and generally, it is also associated with an industrial setting. This might have been true in the past but with the massive adoption of innovation by the public and the rapid technology advancements of the recent decades, the technologies that have made robots useful for the industry are being applied to commercial and residential use cases. The International Organization for Standardization (ISO) defines \u201cservice robots\u201d as \u201crobots that perform useful tasks for humans or equipment, but not for industrial automation applications\u201d. Service robots are very useful when you want to guarantee consistency in a task that involves humans. Humans can react to external factors in many different ways and consistency is hard to achieve, this is especially true in mundane jobs that are highly repetitive (Karunasena et al, 2021.). As robots moved from an industrial setting to performing tasks that are useful to humans, a necessity arose to include social interaction capabilities. If a robot cannot be accepted and be proficient in social scenarios it would be really difficult for them to be useful in applications involving humans. Humans are used to heads being the focal point when interacting with other humans, thus it is expected from robots to possess a responsive social interface to act as this focal point (McGinn, 2019). This project aims to create that interaction focal point while at the same time providing all the features necessary to monitor the robot during contests and development/debugging.","title":"Introduction"},{"location":"#development","text":"The current version of the @Home software is developed using ROS as an overarching framework and both C++ and Python as languages for the individual subsystems. As previously stated, the HRI interface must be able to have deep introspection of the robot subsystems, this means that at least one of the components must run within the ROS environment or have access to the streams of information generated in it. As we\u2019ll see in the next section, this requirement led us to divide the HRI subsystem in several independent parts and deal with the challenges that this brought.","title":"Development"},{"location":"#system-architecture","text":"The system is composed of 3 independent components: An interface developed as a web application, a web server from which the interface retrieves the information to show and finally a ROS node that collects the data from the other subsystems and acts as a database for the web server. Figure 1 shows a high level overview of this architecture and how the communication between them is performed. There\u2019s two main communication scenarios that are showcased with the arrows: from any of the submodules of the robot to the HRI interface, or from the interface to one of the submodules. For the first case, the interface receives each and every message through the use of web sockets, for the second case, the interface sends messages through standard HTTP requests. In any of the two scenarios, the web server establishes communication with the HRI ROS node through the use of Listener and Client primitives that are implemented in the multiprocessing standard Python library, this provides process-to-process communication with built-in serialization using TCP/IP as the underlying protocols. The communications between the HRI ROS node and any other nodes that host different subsystems is done through ROS primitives, meaning using standard or custom ROS messages and by publishing to predefined topics. Figure 1. High level HRI system architecture The reason why the web app interface has to be separate from the other components is fairly obvious, the fact that it is built with a library meant to create SPAs (Single Page Applications) means that it\u2019s supposed to be served independently from the API. However, this does not explain why the web server (the API) has to be a separate process from the ROS node. Both of the components that were just mentioned were written in Python because of its ease of use and our previous familiarity with the language; Python is an interpreted language with dynamic typing which comes with its own set of pros and cons. An interpreted language, as opposed to a compiled language, is not directly executed by the CPU, instead it is either converted to an intermediate representation or kept the same, but in both cases it is executed by an interpreter which is also sometimes called a VM (virtual machine). With all of this said, the root cause of the separation is a mechanism of the Python interpreter called GIL (Global Interpreter Lock) which only allows the code of one thread to be executed concurrently; If ROS and Flask (the framework used to develop the web server) used the same concurrency model it probably wouldn\u2019t have been a problem. However, ROS uses multithreading for concurrency while Flask and almost all other web frameworks for Python use an event loop; these models clash and thus have to be isolated in different processes.","title":"System Architecture"},{"location":"#web-application-interface","text":"The interface is to be run in one of the computers that forms the robot\u2019s computing cluster, most likely running linux; any technology that allows building user interfaces and communication with a web server could suffice. We went with web technologies because of its portability which would allow us to run it in any other machine should it or its operating system change in the future; specifically, we opted to use ReactJS because of its simplicity and wide adoption in the community. Figure 2 shows the main and only page of the interface which is divided into different and independent UI modules. Each module is responsible for receiving information from the web server and displaying it when new updates are available. Each module occupies a flexible amount of screen size and is placed on a flowing grid that allows an arbitrary number of elements to be placed comfortably on the available space. Figure 2. Interface with all the current modules active. The current modules and their corresponding functionalities are listed in the table below. It is worth noting that the ability to easily add future modules is one of the core principles that guided the design of this system, meaning this table should grow easily as the needs of the operators and/or developers change. Name Functionality Communication direction Robot Face Emotionally reflect the actions that the robot is performing. From ROS to the interface. Video feed Show a direct or manipulated (labels, bounding boxes, etc.) feed of video. From ROS to the interface. System health Show the system status of the computing unit in the cluster that is running the interface. This could be extended to show the status of all the computing units. From ROS to the interface. Active robot modules Show which of the robot modules are currently doing processing. From ROS to the interface. Robot Chat Show a log of the text-to-speech messages the robot has emitted. Send custom commands bypassing the speech-to-text module. Both ways. Stop button Send a signal to the robot in case a manual stop is required. From the Interface to ROS. Table 1. UI modules of the HRI interface.. Figure 3. Interface drawer. Table 1 shows all the available modules and while all of them can certainly be on the screen at the same time, there\u2019s no need to have them all; figure 3 shows the system drawer that slides open when the bottom right button shown in figure 2 is pressed. The drawer allows you to select the modules that you wish to see on the main screen as well as creating new profiles or selecting existing ones; a profile is a locally saved configuration that groups several modules together, meaning, when a profile is selected all the modules it groups are shown on the interface while the ones it doesn\u2019t group are hidden. Figure 4. Profile selection UI. Figure 4 shows the UI that can be used to select one of the existing profiles; the interaction is really simple, the dropdown is clicked and when the profile is selected, only the UI modules corresponding to it are shown. When the Add config button is clicked, the modal shown in figure 5 is shown. Figure 5. Profile creation UI. The process to create a profile is very simple: First a name is chosen and then the modules that the profile is going to group must be selected, all or none modules can be selected without any limitations.","title":"Web application interface"},{"location":"#web-server","text":"As mentioned previously, the web server is written in python and uses Flask as the framework of choice. It has a direct connection with the ROS node which can be observed in figure 6. # Full duplex connection with the HRI ros node. receiver_address = ('localhost', 7000) sender_address = ('localhost', 7001) Figure 6. Variables definition for full duplex communication from the server side. To establish a full duplex (or bidirectional) communication with the ros node we have to have both a sender and a receiver and that\u2019s precisely what is showcased in figure 6. The port used for the ROS receiver is 7000 while the port for the sender is 7001, both of these are ports in the local network, meaning in the same compute unit of the robot\u2019s cluster. Table 1 also shows that the stop button requires communication from the interface to the ROS subsystems and for this, the communication has to go through the web server in which HTTP is used. Figure 7 shows the route definition that provides the stop functionality. The interface sends a post request to the /stop endpoint and the web server uses the sender (which previously went through an initialization process) to tell the ros node to shut itself down. @app.route(\"/stop\", methods=[\"POST\"]) def stop_robot(): global ros_sender ros_sender.send(\"shutdown\") return jsonify({\"message\": \"Robot was shutdown\"}) Figure 7. Endpoint definition for the stop button. The other purpose of the web server is message forwarding in the opposite direction, meaning from the ROS subsystems to the interface. This is a well defined task that is performed in the function ros_receive_handler which is showcased in figure 8; the function runs as a task that is dispatched to the event loop (the concurrency models were explained previously). Since the ROS receiver is not initialized immediately, there\u2019s a mechanism against lockup that allows all events running in the loop to progress. The important part of this definition is the else clause of the inner if statement: Whenever the web server receives a message from the ROS node, it analyzes is content and in particular, when a certain format is followed, the web socket channel and the value to be sent are extracted and then emitted through the socket itself. # Decode messages and send them to the appropriate socket channel. def ros_receive_handler(): while True: if ros_receiver is None: # Avoid blocking the process before initialization. socketio.sleep(0.01) continue # Only call `recv` when we're sure there's a new message since # it is a blocking call. if ros_receiver.poll(): try: message = ros_receiver.recv() except: # Ignore message if there was an exception. # TODO: We should not do this becuase we could # loose critical messages. rospy.loginfo(\"There was an exception\") continue if message == \"CreateSender\": initialize_ros_sender() elif message == \"Close\": cleanup() else: channel = message[\"channel\"] value = message[\"value\"] socketio.emit( channel, value if type(value) == str else json.dumps(value) ) else: # Only throttle if there are no available messages. socketio.sleep(0.01) Figure 8. Definition of the event forwarding mechanism.","title":"Web server"},{"location":"#ros-node","text":"As previously mentioned, the role of the ROS node is to compile the information coming from the subsystems and make it easily available to the interface through the web server. In this node we also need the implementation of the full duplex communication; figure 9 shows that the definition is exactly the same, as are the ports to be used, the difference is that from the ROS side, the ports are interchanged: Port 7000 for the server is the receiver while in ROS is the sender; port 7001 for the server is the sender while in ROS is the receiver. # Implement a full duplex connection with the flask server. sender_address = ('localhost', 7000) receiver_address = ('localhost', 7001) Figure 9. Variables definition for full duplex communication from the ROS side. Similar to the server process, the ROS node has a message handler for anything that\u2019s coming from the web server that needs to be handled; for the current implementation, the only event that is handled by ROS coming from the server is shutdown. The implementation for this handler is shown in figure 10, as opposed to the server, the concurrency model in ROS is multithreaded and thus, flask_receive_handler is scheduled as a thread. def flask_receive_handler(): while True: if server_receiver is None: # Don't block execution. time.sleep(0.01) continue if server_receiver.poll(): try: message = server_receiver.recv() if message == \"shutdown\": rospy.loginfo(\"Stopping system\") rospy.loginfo(message) except: break # Don't block execution. time.sleep(.01) Figure 10. Message handler definition for messages going from the server to ROS. The types of messages that the interface currently handles could be classified in two: Robot status and video feed. The video feed is easily represented by a series of Image ROS messages; the definition for such messages is exported by the sensor_msgs package which is widely used by the community. In the case of robot status we created a custom message that encodes all the information needed by the interface (other than images), this definition is shown in figure 11. Header header string[] ActionQueue float32[3] SystemHealth string RobotStatus string[] ActiveModules string RobotMessage string RobotFace Figure 11. Definition for the RobotStatus custom message. As it has been previously mentioned, other than from the server, the ROS node must receive messages from several robot subsystems, and this is done differently for the two discussed cases (status or video). Figure 12 shows how the messages regarding status are handled, which if we analyze it carefully, is a simple forward: We extract the information from the RobotStatus message and send it to the web server. This function is used as the callback of a ROS subscription for the /robo_info topic, which internally is launched as a separate thread. def robot_info_receive_handler(robot_status): if server_sender is not None: for channel in channels: try: server_sender.send({ \"channel\": channel, \"value\": getattr(robot_status, channel) }) except: print(\"Error fetching: \", channel) ... channels = [ \"ActionQueue\", \"SystemHealth\", \"RobotStatus\", \"ActiveModules\", \"RobotMessage\", \"RobotFace\" ] Figure 12. Message handler for robot status The process for the camera feed is very similar, in this case the topic used is /robot_video_feed but there\u2019s some data transformation required before being forwarded to the server, all of this is showcased in figure 13. Originally, the stream arrives as Image messages but has to be encoded as base64; any and all messages sent to the interface through the web sockets must be in string form, either JSON encoded or as plain string data. Base 64 was chosen because browsers support it directly, that is to say, if we can send images as base64 encoded binaries, the browser that is rendering the interface can show them without any conversion. Luckily, OpenCV (which is already used in the robot) has built-in functions to obtain the binary value of an image which can in turn be encoded using Python standard libraries. def robot_video_feed_receive_handler(image_msg): if server_sender is not None: img = bridge.imgmsg_to_cv2(image_msg, desired_encoding=\"passthrough\") _, img_buffer = cv2.imencode('.jpg', img) img_text = base64.b64encode(img_buffer) server_sender.send({ \"channel\": \"CameraFeed\", \"value\": img_text.decode(\"ascii\") }) Figure 13. Message handler for the camera feed.","title":"ROS Node"},{"location":"#future-work","text":"The present work has shown the architecture and implementation of an interface that serves as a proof of concept that such systems can be built and also as a first functional implementation, this because most of the requirements of the development team were met, at least referring to the interface. The future work for the HRI system is very clear: Integrate the interface with the ROS subsystems that can make use of it, specifically, create internal tasks in the nodes so they can send information they see fit to either the /robot_inf or /robot_video_feed topics. Additionally, new UI modules can be added to the interface as needs arise. However, this requires a complete process involving all 3 of the modules. Future iterations of the interface could make the process to add modules easier or even automated through manual scripts or code generation techniques.","title":"Future work"},{"location":"#conclusion","text":"We believe that the way in which we interact with technology and in this case, how we interact with robots, will be a very big research and development area in the future. Although there are multiple classes of robots that can benefit from a better or more robust interaction, the service robot category will certainly receive more attention since there\u2019s still a lot to improve. The purpose of this project was to prove that these types of systems can be built with current technology, for the case of the @Home robot this meant using ROS, Python and web technologies. With a few hiccups along the way, we ended up with a functional first version that paves the way for future development by the end users themselves (the Roborregos team). While the work performed here showcases a very small part of the problems to be solved in this discipline (integration of current technologies), it sets the precedents for future development and showcases the importance of having a system of such kind.","title":"Conclusion"},{"location":"#references","text":"Karunasena, Ramesha et al. \u201cDEVI: Open-Source Human-Robot Interface for Interactive Receptionist Systems.\u201d 2019 IEEE 4th International Conference on Advanced Robotics and Mechatronics (ICARM) (2019): n. pag. Crossref. Web. McGinn, Conor. \u201cWhy Do Robots Need a Head? The Role of Social Interfaces on Service Robots.\u201d International Journal of Social Robotics, vol. 12, no. 1, 2020, p. 281. EBSCOhost, doi:10.1007/s12369-019-00564-5. Iocchi, L. (. 1. )., et al. \u201cRoboCup@Home: Analysis and Results of Evolving Competitions for Domestic and Service Robots.\u201d Artificial Intelligence, vol. 229, pp. 258\u2013281. EBSCOhost, doi:10.1016/j.artint.2015.08.002. Accessed 10 June 2021. Stalljann, Sarah et al. \u201cPerformance Analysis of a Head and Eye Motion-Based Control Interface for Assistive Robots.\u201d Sensors 20.24 (2020): 7162. Crossref. Web. Chen, Mingxuan, et al. \u201cA Human\u2013robot Interface for Mobile Manipulator.\u201d Intelligent Service Robotics, vol. 11, no. 3, 2018, p. 269. EBSCOhost, doi:10.1007/s11370-018-0251-3.","title":"References"},{"location":"user_manual/","text":"User manual The system is composed of the following sections: HRI sub-system: This is what will be covered in this docs. 1. React Web App: Front end were data received from the robot is displayed in a user friendly way. 2. Flask API: Intermediary between ROS Nodes and React Frontend. 3. ROS Node: ROS Node that subscribes to topics that publish robot data. Other robot nodes: These nodes are part of the system of the robot. E.g: Main engine node Navigation Node Speech node Test publisher: This component consists of a node publishing mock data for the ROS Node Listener to receive. React Web App This module is implemented using a React JS. The UI is quite simple, it consists of a series of modular components. A component purpose is to display data from the robot in a user friendly way. Image 1.0 Main UI view In the green enclosed area of Image 1.0 the user can find example of components: Camera Feed System Health Active Robot Modules In the pink enclosed area of Image 1.0 the user can find a button to open the Configuration Drawer . Configuration Drawer In the configuration drawer the user can perform the following actions: Activating components Select a configuration Create a configuration Edit a configuration Image 1.1 Configuration Drawer Activating components In the collapsed drawer that can be seen in Image 1.1 , the user can find a checkbox for each implemented component . All components that are checked are displayed in the GUI. To activate or deactivate a module the user has to check the respective checkbox. Select a configuration In the configuration drawer ( Image 1.1 ) the user can find a dropdown menu button, when collapsed will display a series of options ( Image 1.2 ). Image 1.2 Configuration Selection In the dropdown content displayed in Image 1.2 the user can find a list of current available configurations, the user can create configurations . When the user selects a configuration the GUI will refresh and display the modules that are part of the selected configuration. Create a configuration Image 1.3 Create a configuration At the bottom of the dropdown content ( Image 1.2 ) displays a button for adding a config, when clicked a modal with a form for creating a config will be displayed ( Image 1.3 ). Provide a name for the configuration and check the modules that are part of the configuration and then click the create button. After this the new configuration will be available for selection . Edit a configuration When a configuration is selected it can be edited by modifying the activated modules. As we can observe in Image 1.1 the current configuration is Mariano and it has 3 active modules, by checking/unchecking modules the user can modify the configuration. When the user is done modifying the configuration it can be saved by clicking the Save button enclosed by the configuration selection dropdown ( Image 1.1 ). After clicking the configuration will be updated. Components In the main UI view the user can find examples of components. A component must have an specific purpose, examples are: Camera Feed : Displays camera feed received from the robot. System Health : Displays information about battery, cpu and ram usage. Active Robot Modules : Displays current active robot modules. The following code is how a template component template looks like: import { Box, Text } from \"@chakra-ui/layout\"; import React, { useState, useEffect } from \"react\"; import { socket } from \"../services/socketConnection\"; export default function ComponentTemplate() { // State variables used for component with initial state as null. const [var, setVar] = useState(null); // Use effect for receiving robot socket messages. useEffect(() => { socket.on(\"ComponentTemplate\", setVar); return () => socket.off(\"ComponentTemplate\"); }, []); return ( <Box shadow=\"lg\" p=\"4\" borderRadius=\"lg\" maxW=\"600px\"> <Text fontSize=\"2xl\" mb=\"4\"> Hello World! {var} </Text> </Box> ); } The purpose of a component is to display information received from the robot in a user friendly way. Because of this, it is essential to establish a communication channel with the robot. Components are stored in this directory . The code underlined in green defines a state variable, the useState method returns a variable and a function call to set the variable, when this function is called the component will render again showing changes made by the variable. This variable purpose is to contain the information that is going to be displayed in the component. (More about react state variables at: https://es.reactjs.org/docs/hooks-state.html ) The code underlined in yellow uses a useEffect hook, that is going to execute the code block every time a message is received through the socket, this socket is used to receive messages from the Flask API. (More about react use effect hook at: https://es.reactjs.org/docs/hooks-effect.html ) The code underlined in blue defines the content of the component using JSX, the example shows a simple Hello World. Note that the var can be used in the content to display the information. (More about jsx at: https://es.reactjs.org/docs/introducing-jsx.html ) Examples The following components are currently implemented: Robot Face Camera Feed System Health Robot Chat Active Robot Modules RobotFace The robot face is an interactive set of eyes (Image 1.6) that display emotions, this can be used as a way to interact with the robot that is familiar to humans. Image 1.4 Neutral Robot Face The code can be found here . The face can display the following emotions: Happy Sad Angry Focused Confused Image 1.5 Happy Robot Face Image 1.6 Sad Robot Face Image 1.7 Angry Robot Face Image 1.8 Focused Robot Face Image 1.9 Confused Robot Face The idea is that the user defines the emotion response depending on what the robot is doing. The component receives the emotion through a socket using a hook: useEffect(() => { socket.on(\"RobotFace\", setEmotion); return () => socket.off(\"RobotFace\"); }, []); For example: Confused: This emotion can be sent to the component when the Robot did not understand an instruction. Focused: This emotion can be sent when the Robot understands the current instruction. Angry: This emotion can be sent when the Robot bumped into an obstacle. Happy: This emotion can be sent when the Robot receives a compliment. Essentially the user defines what situations correspond to an emotion, this is sent from a ROS Node which is then read by the Flask API, this API ultimately sends it to the UI via sockets. This is a great tool for debug, because it is very simple to understand and is user defined. It also supports two more settings: start: The robot will start blinking stop: The robot will stop blinking Camera Feed Image 2.0 Camera Feed Component This component displays the camera feed that the robot sends through the system. The code can be found here . The image is received through the socket: useEffect(() => { socket.on(\"CameraFeed\", setImage); return () => socket.off(\"CameraFeed\"); }, []); Then it is displayed in the HTML content of the component: return ( <Box shadow=\"lg\" p=\"4\" borderRadius=\"lg\" maxW=\"600px\"> <Text fontSize=\"2xl\" mb=\"4\"> Camera feed </Text> {image && <img src={`data:image/jpg;base64,${image}`} />} </Box> ); System Health This component displays data about system health from the robot. Image 2.1 System Health Component For now it displays the following information: Battery CPU RAM More types of data can be added, the code can be found here . The data is received from the robot: useEffect(() => { socket.on(SocketChannels.SYSTEM_HEALTH, (data) => setSystemHealth(JSON.parse(data)) ); return () => socket.off(SocketChannels.ACTIVE_MODULES); }, []); And then it is used to display it a user friendly way: return ( <Box shadow=\"lg\" p=\"4\" borderRadius=\"lg\" maxW=\"600px\"> <Text fontSize=\"2xl\" mb=\"4\"> System Health </Text> <VStack divider={<StackDivider borderColor=\"gray\" />} alignItems=\"start\"> <Box> <Icon boxSize=\"1.5em\" mr=\"5\" as={FaBatteryFull} /> Battery:{\" \"} {parseInt(systemHealth[0])}% </Box> <Box> <Icon boxSize=\"1.5em\" mr=\"5\" as={FaPercentage} /> CPU:{\" \"} {parseInt(systemHealth[1])}% </Box> <Box> <Icon boxSize=\"1.5em\" mr=\"5\" as={FaMicrochip} /> RAM:{\" \"} {parseInt(systemHealth[2])}% </Box> </VStack> </Box> ); Robot Chat This component was implemented using React Chatbox component , its functionality is to send and receive text messages to or from the robot. Image 2.2 Robot Chat component Message history is stored in the browser local storage. Messages are received from the robot through the socket: useEffect(() => { socket.on(\"RobotMessage\", setNewMessage); return () => socket.off(\"RobotMessage\"); }, []); const storedMessages = getLocalStorageRobotChat(); if (newMessage != null && !messageExists(newMessage.id, storedMessages)) { console.log(\"New Message: \", newMessage); storedMessages.push(newMessage); setLocalStorageRobotChat(storedMessages); } When a message is received it is verified that it hasn't been received yet, then it is stored in the history and the Chatbox component renders all messages: return ( <Box borderRadius=\"lg\" p=\"4\" shadow=\"lg\" w=\"600px\"> <Text fontSize=\"2xl\" mb=\"4\"> Robot messages </Text> <ChatBox messages={storedMessages} user={user} onSubmit={sendMessage} /> </Box> ); When the user sends a message using the Chatbox component, this message is stored in the history and then it can be sent to the robot using the HRI system by sending a POST request to the Flask API. This input can be used as raw input for the robot. function sendMessage(message) { const newMessage = { text: message, id: uuidv4(), sender: { uid: user.uid, avatar: \"https://img.icons8.com/cotton/2x/gender-neutral-user--v2.png\", }, }; // TODO: Add post to python API that communicates with ROS Node // to send instruction. setNewMessage(newMessage); } Active Robot Modules This component displays which modules from the robot are currently active. Image 2.3 Active Robot Modules component The code can be found here . The data is received from the robot through the socket: useEffect(() => { socket.on(SocketChannels.ACTIVE_MODULES, (data) => setActiveRobotModules(JSON.parse(data)) ); return () => socket.off(SocketChannels.ACTIVE_MODULES); }, []); Then the data is displayed in a friendly way: return ( <Box shadow=\"lg\" p=\"4\" borderRadius=\"lg\" maxW=\"600px\"> <Text fontSize=\"2xl\" mb=\"4\"> Active robot modules </Text> <VStack divider={<StackDivider borderColor=\"gray\" />} alignItems=\"start\"> {Object.values(RobotModule).map((robotModule) => { const active = activeRobotModulesMap[robotModule]; return ( <Box key={robotModule} color={active ? \"green\" : \"gray\"}> {active ? ( <Icon as={FaCheckCircle} /> ) : ( <Icon as={FaTimesCircle} /> )}{\" \"} {mapRobotModuleToName(robotModule)} </Box> ); })} </VStack> </Box> ); Creating a component Creating a component is possible by following two simple steps: Adding a template Adding mapping methods and includes Note that following these steps will only implement the component in the Web App (frontend). It is still necessary to implement the backend part , this consists of data collection and sending. Adding a template This step consists of adding the component React JSX file under this directory . A barebone component looks like this: import { Box, Text } from \"@chakra-ui/layout\"; import React, { useState, useEffect } from \"react\"; import { socket } from \"../services/socketConnection\"; export default function ComponentTemplate() { // State variables used for component with initial state as null. const [variable, setVariable] = useState(null); // Use effect for receiving robot socket messages. useEffect(() => { socket.on(\"ComponentTemplate\", setVariable); return () => socket.off(\"ComponentTemplate\"); }, []); return ( <Box shadow=\"lg\" p=\"4\" borderRadius=\"lg\" maxW=\"600px\"> <Text fontSize=\"2xl\" mb=\"4\"> Hello World! {variable} </Text> </Box> ); } ComponentTemplate has to be substituted with the name of your component. Setup receiving messages through the socket in the appropiate channel (backend sending messages still has to be implemented): // Use effect for receiving robot socket messages. useEffect(() => { socket.on(\"ComponentTemplate\", setVar); return () => socket.off(\"ComponentTemplate\"); }, []); And then design the structure of the component: return ( <Box shadow=\"lg\" p=\"4\" borderRadius=\"lg\" maxW=\"600px\"> <Text fontSize=\"2xl\" mb=\"4\"> Hello World! {variable} </Text> </Box> ); Adding mapping methods and includes For the component to display in the UI and configuration drawer it has to included in couple of files. All components are refered to using an enum called ModuleIdentifier . The component name has to be added the following way: export const ModuleIdentifier = { FACE: \"face_module\", CAMERA: \"camera_module\", SYSTEM_HEALTH: \"system_health_module\", ROBOT_MODULES: \"robot_modules\", CHAT: \"chat_module\", NEW_COMPONENT_NAME: \"new_component_name\" }; In the util library are located a couple of methods that provide the interface to display components in the UI. In this file we have to include our component template: import CameraFeed from \"../components/CameraFeed\"; import RobotChat from \"../components/RobotChat\"; import RobotFace from \"../components/RobotFace/RobotFace\"; import SystemHealthModule from \"../components/SystemHealthModule\"; import RobotModulesModule from \"../components/RobotModulesModule\"; import NewComponentName from \"../components/NewComponentName\"; Inside this library we can find two methods mapping methods that we have to add our new component: mapModuleIdToComponent This method maps the module Id (enum value) to the React component, the component has to be added the following way: export function mapModuleIdToComponent(moduleId) { switch (moduleId) { case ModuleIdentifier.CAMERA: return <CameraFeed />; case ModuleIdentifier.CHAT: return <RobotChat />; case ModuleIdentifier.FACE: return <RobotFace />; case ModuleIdentifier.SYSTEM_HEALTH: // TODO: implement system health. return <SystemHealthModule />; case ModuleIdentifier.NEW_COMPONENT_NAME: return <NewComponentName />; case ModuleIdentifier.ROBOT_MODULE default: // Return an empty div if the value is not recognized. return <div />; } } If the module id is not handled an empty div will be returned. mapModuleIdToName This method is for displaying the name of the component in the UI, it maps the module id to the name. The component has to be added the following way: export function mapModuleIdToName(moduleId) { switch (moduleId) { case ModuleIdentifier.CAMERA: return \"Camera feed\"; case ModuleIdentifier.CHAT: return \"Robot chat\"; case ModuleIdentifier.FACE: return \"Face\"; case ModuleIdentifier.SYSTEM_HEALTH: return \"System health\"; case ModuleIdentifier.NEW_COMPONENT_NAME: return \"NewComponentName\"; default: return \"Unknown module\"; } } If the module id is not handled \"Unknown module\" will be shown as name. Flask API The Flask API consists of the following modules: Sockets : SocketIO library is used to establish communication and send messages to the React Web App Web endpoints : This can be get or post depending on the needs and can be used to send messages from the React Web App to ROS Node Connection to ROS Node : Full duplex connection with ROS Node . The code can be found here . This API works as intermediary between the frontend and backend because there is no direct way that both can communicate because of how ROS works. Sockets The purpose of this module is to send the collected data to the React Web App . The data is not created at this point, it is received from the ROS Node using the TCP/IP server . When data is received it is only redirected to the frontend using SocketIO: channel = message[\"channel\"] value = message[\"value\"] socketio.emit( channel, value if type(value) == str else json.dumps(value) ) Web endpoints A user can implement an endpoint in the flask server for receiving web data from the UI. e.g. the stop endpoint: @app.route(\"/stop\", methods=[\"POST\"]) def stop_robot(): global ros_sender ros_sender.send(\"shutdown\") return jsonify({\"message\": \"Robot was shutdown\"}) Then the programmer can define what to do with the data, like sending it to the ROS Node using the TCP/IP client . Connection to ROS Node This is possible by using a TCP/IP server and client in both the Flask API and ROS Node, so that both modules can send and receive data. In the following piece of code received messages are handled: # Decode messages and send them to the appropriate socket channel. def ros_receive_handler(): while True: if ros_receiver is None: # Avoid blocking the process before initialization. socketio.sleep(0.01) continue # Only call `recv` when we're sure there's a new message since # it is a blocking call. if ros_receiver.poll(): try: message = ros_receiver.recv() except: # Ignore message if there was an exception. # TODO: We should not do this becuase we could # loose critical messages. rospy.loginfo(\"There was an exception\") continue if message == \"CreateSender\": initialize_ros_sender() elif message == \"Close\": cleanup() else: channel = message[\"channel\"] value = message[\"value\"] socketio.emit( channel, value if type(value) == str else json.dumps(value) ) else: # Only throttle if there are no available messages. socketio.sleep(0.01) Note that when a message that needs to be sent to the UI is received it is done using sockets . Messages can be sent to using ros_sender . Example in the /stop endpoint: @app.route(\"/stop\", methods=[\"POST\"]) def stop_robot(): global ros_sender ros_sender.send(\"shutdown\") return jsonify({\"message\": \"Robot was shutdown\"}) When anything is posted to this endpoint a shutdown signal is sent to ROS Node using the client. This needs to be handled by the ROS Node appropriately. ROS Node This module consists of a ROS Node called hri. The code can be found here . The purpose of this node is to listen to messages published to topics related to debugging and then send the data to the flask API using the TCP/IP client so that it can be sent ultimately to the React Web App . Currently the node is subscribing to the following topics: Robot Info Robot Video Feed Robot Info Subscription is made to topic /robot_info ( source ) where RobotStatus messages are published. The structure of RobotStatus: Header header string[] ActionQueue float32[3] SystemHealth string RobotStatus string[] ActiveModules string RobotMessage string RobotFace The following code executes when a new RobotStatus is received: def robot_info_receive_handler(robot_status): if server_sender is not None: for channel in channels: try: server_sender.send({ \"channel\": channel, \"value\": getattr(robot_status, channel) }) except: print(\"Error fetching: \", channel) Where channels is defined as: channels = [ \"ActionQueue\", \"SystemHealth\", \"RobotStatus\", \"ActiveModules\", \"RobotMessage\", \"RobotFace\" ] Note that channels content is the same as the data names defined for RobotStatus, this name must also match the channel name used when listening to the socket, because it will be used also for sending through the socket. Sending is made by the flask server . Example of receiving through the socket: useEffect(() => { socket.on(SocketChannels.SYSTEM_HEALTH, (data) => setSystemHealth(JSON.parse(data)) ); return () => socket.off(SocketChannels.SYSTEM_HEALTH); }, []); ... export const SocketChannels = { SYSTEM_HEALTH: \"SystemHealth\", ACTIVE_MODULES: \"ActiveModules\", }; Note that SystemHealth matches the name used by the ROS Node from channels and RobotStatus message. Robot Video Feed Subscription is made to topic /robot_video_feed ( source ) where Video messages are published. This was implemented as a different topic than /robot_info so that video feed can be handled independently from simple debug data. The following code executes when a new video feed is published: def robot_video_feed_receive_handler(image_msg): if server_sender is not None: img = bridge.imgmsg_to_cv2(image_msg, desired_encoding=\"passthrough\") _, img_buffer = cv2.imencode('.jpg', img) img_text = base64.b64encode(img_buffer) server_sender.send({ \"channel\": \"CameraFeed\", \"value\": img_text.decode(\"ascii\") }) It is then sent to the Flask API so that it can be sent ultimately to the React Web App . Example of this message being used in the UI: export default function CameraFeed() { const [image, setImage] = useState(null); useEffect(() => { socket.on(\"CameraFeed\", setImage); return () => socket.off(\"CameraFeed\"); }, []); return ( <Box shadow=\"lg\" p=\"4\" borderRadius=\"lg\" maxW=\"600px\"> <Text fontSize=\"2xl\" mb=\"4\"> Camera feed </Text> {image && <img src={`data:image/jpg;base64,${image}`} />} </Box> ); } Note that 'CameraFeed' is being used to listen for image feed using the socket. Mote at Camera Feed . Test Node Publisher This module is not part of the system but is used to test it and show an example on how to collect and send data to the UI. The code can be found here . This module consists of a ROS Node called test_hri_publisher that publishes to two topics: /robot_info /robot_video_feed The same flow must be followed to publish messages for the UI to show, but collected data must be real and where, when and why it is published is determined by the user. Robot Info The following is executed periodically publishing to /robot_info . def status_publisher(): rospy.loginfo(\"Starting status publisher\") info_pub = rospy.Publisher(\"/robot_info\", RobotStatus, queue_size=10) rate = rospy.Rate(1/3) try: while not rospy.is_shutdown(): # Publish robot status. message = RobotStatus() # Set active modules. active_robot_modules = [] for module in robot_modules: if bool(random.getrandbits(1)): active_robot_modules.append(module) message.ActiveModules = active_robot_modules # Set system health. message.SystemHealth = [ # Battery. float(85 + int(random.getrandbits(4))), # CPU. float(random.getrandbits(4)), # RAM. float(random.getrandbits(4)), ] # TODO: add values to the rest of the message. info_pub.publish(message) rate.sleep() except: pass Remembering RobotStatus structure: Header header string[] ActionQueue float32[3] SystemHealth string RobotStatus string[] ActiveModules string RobotMessage string RobotFace ActiveModules field is an array of the currently active robot modules from the following list: robot_modules = [ \"speech\", \"nlu\", \"navigation\", \"object_recognition\", \"person_recognition\", \"mechanism_control\", \"main_engine\" ] The test publisher randomly select robot modules to be markes as active in the UI. This is then sent to the Flask API to be redirected to the UI using sockets. In this component an example of how the data obtained through the socket is used to render the active modules in a user friendly way. The case of SystemHealth can also be found in the test publisher. This field is defined as an array of floats of size 3, and it is also filled with random data: # Set system health. message.SystemHealth = [ # Battery. float(85 + int(random.getrandbits(4))), # CPU. float(random.getrandbits(4)), # RAM. float(random.getrandbits(4)), ] This data is then sent to the UI through the system and used in the System Health Component Robot Video Feed This publisher reads from the camera using open cv and then publishes it to robot_video_feed topic. def video_publisher(): rospy.loginfo(\"Starting video publisher\") image_pub = rospy.Publisher(\"/robot_video_feed\", Image, queue_size=10) rate = rospy.Rate(10) try: while not rospy.is_shutdown(): # Publish video feed. _, img = vid.read() image_message = bridge.cv2_to_imgmsg(img, encoding=\"passthrough\") image_pub.publish(image_message) rate.sleep() except: pass Then it is received by the ROS Node listener and sent to the UI through the Flask Api . Example of how this data is used can be found in the Camera Feed Component . Implementing new component backend This consists of setting up data collection, and setting up the channel to send it. It can be possible by doing this simple stesps: Adding new field to RobotStatus message. More info about supported datatypes here . Adding new field name to channels here . Collecting real time data and publishing it to /robot_info so that it can be sent by the ROS Node to the Flask API and ultimately to the React Web App . This implements only the backend, to setup the front end check out: Creating a component","title":"User manual"},{"location":"user_manual/#user-manual","text":"The system is composed of the following sections: HRI sub-system: This is what will be covered in this docs. 1. React Web App: Front end were data received from the robot is displayed in a user friendly way. 2. Flask API: Intermediary between ROS Nodes and React Frontend. 3. ROS Node: ROS Node that subscribes to topics that publish robot data. Other robot nodes: These nodes are part of the system of the robot. E.g: Main engine node Navigation Node Speech node Test publisher: This component consists of a node publishing mock data for the ROS Node Listener to receive.","title":"User manual"},{"location":"user_manual/#react-web-app","text":"This module is implemented using a React JS. The UI is quite simple, it consists of a series of modular components. A component purpose is to display data from the robot in a user friendly way. Image 1.0 Main UI view In the green enclosed area of Image 1.0 the user can find example of components: Camera Feed System Health Active Robot Modules In the pink enclosed area of Image 1.0 the user can find a button to open the Configuration Drawer .","title":"React Web App"},{"location":"user_manual/#configuration-drawer","text":"In the configuration drawer the user can perform the following actions: Activating components Select a configuration Create a configuration Edit a configuration Image 1.1 Configuration Drawer","title":"Configuration Drawer"},{"location":"user_manual/#activating-components","text":"In the collapsed drawer that can be seen in Image 1.1 , the user can find a checkbox for each implemented component . All components that are checked are displayed in the GUI. To activate or deactivate a module the user has to check the respective checkbox.","title":"Activating components"},{"location":"user_manual/#select-a-configuration","text":"In the configuration drawer ( Image 1.1 ) the user can find a dropdown menu button, when collapsed will display a series of options ( Image 1.2 ). Image 1.2 Configuration Selection In the dropdown content displayed in Image 1.2 the user can find a list of current available configurations, the user can create configurations . When the user selects a configuration the GUI will refresh and display the modules that are part of the selected configuration.","title":"Select a configuration"},{"location":"user_manual/#create-a-configuration","text":"Image 1.3 Create a configuration At the bottom of the dropdown content ( Image 1.2 ) displays a button for adding a config, when clicked a modal with a form for creating a config will be displayed ( Image 1.3 ). Provide a name for the configuration and check the modules that are part of the configuration and then click the create button. After this the new configuration will be available for selection .","title":"Create a configuration"},{"location":"user_manual/#edit-a-configuration","text":"When a configuration is selected it can be edited by modifying the activated modules. As we can observe in Image 1.1 the current configuration is Mariano and it has 3 active modules, by checking/unchecking modules the user can modify the configuration. When the user is done modifying the configuration it can be saved by clicking the Save button enclosed by the configuration selection dropdown ( Image 1.1 ). After clicking the configuration will be updated.","title":"Edit a configuration"},{"location":"user_manual/#components","text":"In the main UI view the user can find examples of components. A component must have an specific purpose, examples are: Camera Feed : Displays camera feed received from the robot. System Health : Displays information about battery, cpu and ram usage. Active Robot Modules : Displays current active robot modules. The following code is how a template component template looks like: import { Box, Text } from \"@chakra-ui/layout\"; import React, { useState, useEffect } from \"react\"; import { socket } from \"../services/socketConnection\"; export default function ComponentTemplate() { // State variables used for component with initial state as null. const [var, setVar] = useState(null); // Use effect for receiving robot socket messages. useEffect(() => { socket.on(\"ComponentTemplate\", setVar); return () => socket.off(\"ComponentTemplate\"); }, []); return ( <Box shadow=\"lg\" p=\"4\" borderRadius=\"lg\" maxW=\"600px\"> <Text fontSize=\"2xl\" mb=\"4\"> Hello World! {var} </Text> </Box> ); } The purpose of a component is to display information received from the robot in a user friendly way. Because of this, it is essential to establish a communication channel with the robot. Components are stored in this directory . The code underlined in green defines a state variable, the useState method returns a variable and a function call to set the variable, when this function is called the component will render again showing changes made by the variable. This variable purpose is to contain the information that is going to be displayed in the component. (More about react state variables at: https://es.reactjs.org/docs/hooks-state.html ) The code underlined in yellow uses a useEffect hook, that is going to execute the code block every time a message is received through the socket, this socket is used to receive messages from the Flask API. (More about react use effect hook at: https://es.reactjs.org/docs/hooks-effect.html ) The code underlined in blue defines the content of the component using JSX, the example shows a simple Hello World. Note that the var can be used in the content to display the information. (More about jsx at: https://es.reactjs.org/docs/introducing-jsx.html )","title":"Components"},{"location":"user_manual/#examples","text":"The following components are currently implemented: Robot Face Camera Feed System Health Robot Chat Active Robot Modules","title":"Examples"},{"location":"user_manual/#robotface","text":"The robot face is an interactive set of eyes (Image 1.6) that display emotions, this can be used as a way to interact with the robot that is familiar to humans. Image 1.4 Neutral Robot Face The code can be found here . The face can display the following emotions: Happy Sad Angry Focused Confused Image 1.5 Happy Robot Face Image 1.6 Sad Robot Face Image 1.7 Angry Robot Face Image 1.8 Focused Robot Face Image 1.9 Confused Robot Face The idea is that the user defines the emotion response depending on what the robot is doing. The component receives the emotion through a socket using a hook: useEffect(() => { socket.on(\"RobotFace\", setEmotion); return () => socket.off(\"RobotFace\"); }, []); For example: Confused: This emotion can be sent to the component when the Robot did not understand an instruction. Focused: This emotion can be sent when the Robot understands the current instruction. Angry: This emotion can be sent when the Robot bumped into an obstacle. Happy: This emotion can be sent when the Robot receives a compliment. Essentially the user defines what situations correspond to an emotion, this is sent from a ROS Node which is then read by the Flask API, this API ultimately sends it to the UI via sockets. This is a great tool for debug, because it is very simple to understand and is user defined. It also supports two more settings: start: The robot will start blinking stop: The robot will stop blinking","title":"RobotFace"},{"location":"user_manual/#camera-feed","text":"Image 2.0 Camera Feed Component This component displays the camera feed that the robot sends through the system. The code can be found here . The image is received through the socket: useEffect(() => { socket.on(\"CameraFeed\", setImage); return () => socket.off(\"CameraFeed\"); }, []); Then it is displayed in the HTML content of the component: return ( <Box shadow=\"lg\" p=\"4\" borderRadius=\"lg\" maxW=\"600px\"> <Text fontSize=\"2xl\" mb=\"4\"> Camera feed </Text> {image && <img src={`data:image/jpg;base64,${image}`} />} </Box> );","title":"Camera Feed"},{"location":"user_manual/#system-health","text":"This component displays data about system health from the robot. Image 2.1 System Health Component For now it displays the following information: Battery CPU RAM More types of data can be added, the code can be found here . The data is received from the robot: useEffect(() => { socket.on(SocketChannels.SYSTEM_HEALTH, (data) => setSystemHealth(JSON.parse(data)) ); return () => socket.off(SocketChannels.ACTIVE_MODULES); }, []); And then it is used to display it a user friendly way: return ( <Box shadow=\"lg\" p=\"4\" borderRadius=\"lg\" maxW=\"600px\"> <Text fontSize=\"2xl\" mb=\"4\"> System Health </Text> <VStack divider={<StackDivider borderColor=\"gray\" />} alignItems=\"start\"> <Box> <Icon boxSize=\"1.5em\" mr=\"5\" as={FaBatteryFull} /> Battery:{\" \"} {parseInt(systemHealth[0])}% </Box> <Box> <Icon boxSize=\"1.5em\" mr=\"5\" as={FaPercentage} /> CPU:{\" \"} {parseInt(systemHealth[1])}% </Box> <Box> <Icon boxSize=\"1.5em\" mr=\"5\" as={FaMicrochip} /> RAM:{\" \"} {parseInt(systemHealth[2])}% </Box> </VStack> </Box> );","title":"System Health"},{"location":"user_manual/#robot-chat","text":"This component was implemented using React Chatbox component , its functionality is to send and receive text messages to or from the robot. Image 2.2 Robot Chat component Message history is stored in the browser local storage. Messages are received from the robot through the socket: useEffect(() => { socket.on(\"RobotMessage\", setNewMessage); return () => socket.off(\"RobotMessage\"); }, []); const storedMessages = getLocalStorageRobotChat(); if (newMessage != null && !messageExists(newMessage.id, storedMessages)) { console.log(\"New Message: \", newMessage); storedMessages.push(newMessage); setLocalStorageRobotChat(storedMessages); } When a message is received it is verified that it hasn't been received yet, then it is stored in the history and the Chatbox component renders all messages: return ( <Box borderRadius=\"lg\" p=\"4\" shadow=\"lg\" w=\"600px\"> <Text fontSize=\"2xl\" mb=\"4\"> Robot messages </Text> <ChatBox messages={storedMessages} user={user} onSubmit={sendMessage} /> </Box> ); When the user sends a message using the Chatbox component, this message is stored in the history and then it can be sent to the robot using the HRI system by sending a POST request to the Flask API. This input can be used as raw input for the robot. function sendMessage(message) { const newMessage = { text: message, id: uuidv4(), sender: { uid: user.uid, avatar: \"https://img.icons8.com/cotton/2x/gender-neutral-user--v2.png\", }, }; // TODO: Add post to python API that communicates with ROS Node // to send instruction. setNewMessage(newMessage); }","title":"Robot Chat"},{"location":"user_manual/#active-robot-modules","text":"This component displays which modules from the robot are currently active. Image 2.3 Active Robot Modules component The code can be found here . The data is received from the robot through the socket: useEffect(() => { socket.on(SocketChannels.ACTIVE_MODULES, (data) => setActiveRobotModules(JSON.parse(data)) ); return () => socket.off(SocketChannels.ACTIVE_MODULES); }, []); Then the data is displayed in a friendly way: return ( <Box shadow=\"lg\" p=\"4\" borderRadius=\"lg\" maxW=\"600px\"> <Text fontSize=\"2xl\" mb=\"4\"> Active robot modules </Text> <VStack divider={<StackDivider borderColor=\"gray\" />} alignItems=\"start\"> {Object.values(RobotModule).map((robotModule) => { const active = activeRobotModulesMap[robotModule]; return ( <Box key={robotModule} color={active ? \"green\" : \"gray\"}> {active ? ( <Icon as={FaCheckCircle} /> ) : ( <Icon as={FaTimesCircle} /> )}{\" \"} {mapRobotModuleToName(robotModule)} </Box> ); })} </VStack> </Box> );","title":"Active Robot Modules"},{"location":"user_manual/#creating-a-component","text":"Creating a component is possible by following two simple steps: Adding a template Adding mapping methods and includes Note that following these steps will only implement the component in the Web App (frontend). It is still necessary to implement the backend part , this consists of data collection and sending.","title":"Creating a component"},{"location":"user_manual/#adding-a-template","text":"This step consists of adding the component React JSX file under this directory . A barebone component looks like this: import { Box, Text } from \"@chakra-ui/layout\"; import React, { useState, useEffect } from \"react\"; import { socket } from \"../services/socketConnection\"; export default function ComponentTemplate() { // State variables used for component with initial state as null. const [variable, setVariable] = useState(null); // Use effect for receiving robot socket messages. useEffect(() => { socket.on(\"ComponentTemplate\", setVariable); return () => socket.off(\"ComponentTemplate\"); }, []); return ( <Box shadow=\"lg\" p=\"4\" borderRadius=\"lg\" maxW=\"600px\"> <Text fontSize=\"2xl\" mb=\"4\"> Hello World! {variable} </Text> </Box> ); } ComponentTemplate has to be substituted with the name of your component. Setup receiving messages through the socket in the appropiate channel (backend sending messages still has to be implemented): // Use effect for receiving robot socket messages. useEffect(() => { socket.on(\"ComponentTemplate\", setVar); return () => socket.off(\"ComponentTemplate\"); }, []); And then design the structure of the component: return ( <Box shadow=\"lg\" p=\"4\" borderRadius=\"lg\" maxW=\"600px\"> <Text fontSize=\"2xl\" mb=\"4\"> Hello World! {variable} </Text> </Box> );","title":"Adding a template"},{"location":"user_manual/#adding-mapping-methods-and-includes","text":"For the component to display in the UI and configuration drawer it has to included in couple of files. All components are refered to using an enum called ModuleIdentifier . The component name has to be added the following way: export const ModuleIdentifier = { FACE: \"face_module\", CAMERA: \"camera_module\", SYSTEM_HEALTH: \"system_health_module\", ROBOT_MODULES: \"robot_modules\", CHAT: \"chat_module\", NEW_COMPONENT_NAME: \"new_component_name\" }; In the util library are located a couple of methods that provide the interface to display components in the UI. In this file we have to include our component template: import CameraFeed from \"../components/CameraFeed\"; import RobotChat from \"../components/RobotChat\"; import RobotFace from \"../components/RobotFace/RobotFace\"; import SystemHealthModule from \"../components/SystemHealthModule\"; import RobotModulesModule from \"../components/RobotModulesModule\"; import NewComponentName from \"../components/NewComponentName\"; Inside this library we can find two methods mapping methods that we have to add our new component:","title":"Adding mapping methods and includes"},{"location":"user_manual/#mapmoduleidtocomponent","text":"This method maps the module Id (enum value) to the React component, the component has to be added the following way: export function mapModuleIdToComponent(moduleId) { switch (moduleId) { case ModuleIdentifier.CAMERA: return <CameraFeed />; case ModuleIdentifier.CHAT: return <RobotChat />; case ModuleIdentifier.FACE: return <RobotFace />; case ModuleIdentifier.SYSTEM_HEALTH: // TODO: implement system health. return <SystemHealthModule />; case ModuleIdentifier.NEW_COMPONENT_NAME: return <NewComponentName />; case ModuleIdentifier.ROBOT_MODULE default: // Return an empty div if the value is not recognized. return <div />; } } If the module id is not handled an empty div will be returned.","title":"mapModuleIdToComponent"},{"location":"user_manual/#mapmoduleidtoname","text":"This method is for displaying the name of the component in the UI, it maps the module id to the name. The component has to be added the following way: export function mapModuleIdToName(moduleId) { switch (moduleId) { case ModuleIdentifier.CAMERA: return \"Camera feed\"; case ModuleIdentifier.CHAT: return \"Robot chat\"; case ModuleIdentifier.FACE: return \"Face\"; case ModuleIdentifier.SYSTEM_HEALTH: return \"System health\"; case ModuleIdentifier.NEW_COMPONENT_NAME: return \"NewComponentName\"; default: return \"Unknown module\"; } } If the module id is not handled \"Unknown module\" will be shown as name.","title":"mapModuleIdToName"},{"location":"user_manual/#flask-api","text":"The Flask API consists of the following modules: Sockets : SocketIO library is used to establish communication and send messages to the React Web App Web endpoints : This can be get or post depending on the needs and can be used to send messages from the React Web App to ROS Node Connection to ROS Node : Full duplex connection with ROS Node . The code can be found here . This API works as intermediary between the frontend and backend because there is no direct way that both can communicate because of how ROS works.","title":"Flask API"},{"location":"user_manual/#sockets","text":"The purpose of this module is to send the collected data to the React Web App . The data is not created at this point, it is received from the ROS Node using the TCP/IP server . When data is received it is only redirected to the frontend using SocketIO: channel = message[\"channel\"] value = message[\"value\"] socketio.emit( channel, value if type(value) == str else json.dumps(value) )","title":"Sockets"},{"location":"user_manual/#web-endpoints","text":"A user can implement an endpoint in the flask server for receiving web data from the UI. e.g. the stop endpoint: @app.route(\"/stop\", methods=[\"POST\"]) def stop_robot(): global ros_sender ros_sender.send(\"shutdown\") return jsonify({\"message\": \"Robot was shutdown\"}) Then the programmer can define what to do with the data, like sending it to the ROS Node using the TCP/IP client .","title":"Web endpoints"},{"location":"user_manual/#connection-to-ros-node","text":"This is possible by using a TCP/IP server and client in both the Flask API and ROS Node, so that both modules can send and receive data. In the following piece of code received messages are handled: # Decode messages and send them to the appropriate socket channel. def ros_receive_handler(): while True: if ros_receiver is None: # Avoid blocking the process before initialization. socketio.sleep(0.01) continue # Only call `recv` when we're sure there's a new message since # it is a blocking call. if ros_receiver.poll(): try: message = ros_receiver.recv() except: # Ignore message if there was an exception. # TODO: We should not do this becuase we could # loose critical messages. rospy.loginfo(\"There was an exception\") continue if message == \"CreateSender\": initialize_ros_sender() elif message == \"Close\": cleanup() else: channel = message[\"channel\"] value = message[\"value\"] socketio.emit( channel, value if type(value) == str else json.dumps(value) ) else: # Only throttle if there are no available messages. socketio.sleep(0.01) Note that when a message that needs to be sent to the UI is received it is done using sockets . Messages can be sent to using ros_sender . Example in the /stop endpoint: @app.route(\"/stop\", methods=[\"POST\"]) def stop_robot(): global ros_sender ros_sender.send(\"shutdown\") return jsonify({\"message\": \"Robot was shutdown\"}) When anything is posted to this endpoint a shutdown signal is sent to ROS Node using the client. This needs to be handled by the ROS Node appropriately.","title":"Connection to ROS Node"},{"location":"user_manual/#ros-node","text":"This module consists of a ROS Node called hri. The code can be found here . The purpose of this node is to listen to messages published to topics related to debugging and then send the data to the flask API using the TCP/IP client so that it can be sent ultimately to the React Web App . Currently the node is subscribing to the following topics: Robot Info Robot Video Feed","title":"ROS Node"},{"location":"user_manual/#robot-info","text":"Subscription is made to topic /robot_info ( source ) where RobotStatus messages are published. The structure of RobotStatus: Header header string[] ActionQueue float32[3] SystemHealth string RobotStatus string[] ActiveModules string RobotMessage string RobotFace The following code executes when a new RobotStatus is received: def robot_info_receive_handler(robot_status): if server_sender is not None: for channel in channels: try: server_sender.send({ \"channel\": channel, \"value\": getattr(robot_status, channel) }) except: print(\"Error fetching: \", channel) Where channels is defined as: channels = [ \"ActionQueue\", \"SystemHealth\", \"RobotStatus\", \"ActiveModules\", \"RobotMessage\", \"RobotFace\" ] Note that channels content is the same as the data names defined for RobotStatus, this name must also match the channel name used when listening to the socket, because it will be used also for sending through the socket. Sending is made by the flask server . Example of receiving through the socket: useEffect(() => { socket.on(SocketChannels.SYSTEM_HEALTH, (data) => setSystemHealth(JSON.parse(data)) ); return () => socket.off(SocketChannels.SYSTEM_HEALTH); }, []); ... export const SocketChannels = { SYSTEM_HEALTH: \"SystemHealth\", ACTIVE_MODULES: \"ActiveModules\", }; Note that SystemHealth matches the name used by the ROS Node from channels and RobotStatus message.","title":"Robot Info"},{"location":"user_manual/#robot-video-feed","text":"Subscription is made to topic /robot_video_feed ( source ) where Video messages are published. This was implemented as a different topic than /robot_info so that video feed can be handled independently from simple debug data. The following code executes when a new video feed is published: def robot_video_feed_receive_handler(image_msg): if server_sender is not None: img = bridge.imgmsg_to_cv2(image_msg, desired_encoding=\"passthrough\") _, img_buffer = cv2.imencode('.jpg', img) img_text = base64.b64encode(img_buffer) server_sender.send({ \"channel\": \"CameraFeed\", \"value\": img_text.decode(\"ascii\") }) It is then sent to the Flask API so that it can be sent ultimately to the React Web App . Example of this message being used in the UI: export default function CameraFeed() { const [image, setImage] = useState(null); useEffect(() => { socket.on(\"CameraFeed\", setImage); return () => socket.off(\"CameraFeed\"); }, []); return ( <Box shadow=\"lg\" p=\"4\" borderRadius=\"lg\" maxW=\"600px\"> <Text fontSize=\"2xl\" mb=\"4\"> Camera feed </Text> {image && <img src={`data:image/jpg;base64,${image}`} />} </Box> ); } Note that 'CameraFeed' is being used to listen for image feed using the socket. Mote at Camera Feed .","title":"Robot Video Feed"},{"location":"user_manual/#test-node-publisher","text":"This module is not part of the system but is used to test it and show an example on how to collect and send data to the UI. The code can be found here . This module consists of a ROS Node called test_hri_publisher that publishes to two topics: /robot_info /robot_video_feed The same flow must be followed to publish messages for the UI to show, but collected data must be real and where, when and why it is published is determined by the user.","title":"Test Node Publisher"},{"location":"user_manual/#robot-info_1","text":"The following is executed periodically publishing to /robot_info . def status_publisher(): rospy.loginfo(\"Starting status publisher\") info_pub = rospy.Publisher(\"/robot_info\", RobotStatus, queue_size=10) rate = rospy.Rate(1/3) try: while not rospy.is_shutdown(): # Publish robot status. message = RobotStatus() # Set active modules. active_robot_modules = [] for module in robot_modules: if bool(random.getrandbits(1)): active_robot_modules.append(module) message.ActiveModules = active_robot_modules # Set system health. message.SystemHealth = [ # Battery. float(85 + int(random.getrandbits(4))), # CPU. float(random.getrandbits(4)), # RAM. float(random.getrandbits(4)), ] # TODO: add values to the rest of the message. info_pub.publish(message) rate.sleep() except: pass Remembering RobotStatus structure: Header header string[] ActionQueue float32[3] SystemHealth string RobotStatus string[] ActiveModules string RobotMessage string RobotFace ActiveModules field is an array of the currently active robot modules from the following list: robot_modules = [ \"speech\", \"nlu\", \"navigation\", \"object_recognition\", \"person_recognition\", \"mechanism_control\", \"main_engine\" ] The test publisher randomly select robot modules to be markes as active in the UI. This is then sent to the Flask API to be redirected to the UI using sockets. In this component an example of how the data obtained through the socket is used to render the active modules in a user friendly way. The case of SystemHealth can also be found in the test publisher. This field is defined as an array of floats of size 3, and it is also filled with random data: # Set system health. message.SystemHealth = [ # Battery. float(85 + int(random.getrandbits(4))), # CPU. float(random.getrandbits(4)), # RAM. float(random.getrandbits(4)), ] This data is then sent to the UI through the system and used in the System Health Component","title":"Robot Info"},{"location":"user_manual/#robot-video-feed_1","text":"This publisher reads from the camera using open cv and then publishes it to robot_video_feed topic. def video_publisher(): rospy.loginfo(\"Starting video publisher\") image_pub = rospy.Publisher(\"/robot_video_feed\", Image, queue_size=10) rate = rospy.Rate(10) try: while not rospy.is_shutdown(): # Publish video feed. _, img = vid.read() image_message = bridge.cv2_to_imgmsg(img, encoding=\"passthrough\") image_pub.publish(image_message) rate.sleep() except: pass Then it is received by the ROS Node listener and sent to the UI through the Flask Api . Example of how this data is used can be found in the Camera Feed Component .","title":"Robot Video Feed"},{"location":"user_manual/#implementing-new-component-backend","text":"This consists of setting up data collection, and setting up the channel to send it. It can be possible by doing this simple stesps: Adding new field to RobotStatus message. More info about supported datatypes here . Adding new field name to channels here . Collecting real time data and publishing it to /robot_info so that it can be sent by the ROS Node to the Flask API and ultimately to the React Web App . This implements only the backend, to setup the front end check out: Creating a component","title":"Implementing new component backend"}]}